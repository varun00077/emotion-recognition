# ğŸ­ Emotion Detection from Speech using CNN-LSTM  

This project detects **human emotions** from speech audio using a **hybrid Convolutional Neural Network (CNN)** and **Bidirectional LSTM** model.  
It is trained on the **RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and Song)** dataset to classify emotions such as  
ğŸ˜ Neutral Â· ğŸ˜Œ Calm Â· ğŸ˜€ Happy Â· ğŸ˜¢ Sad Â· ğŸ˜¡ Angry Â· ğŸ˜± Fearful Â· ğŸ¤¢ Disgust Â· ğŸ˜² Surprised  

---

## ğŸ“˜ Overview  

The system extracts deep audio features (MFCC, Chroma, Spectral Contrast, Tonnetz) using **Librosa**,  
then feeds them into a CNN-LSTM architecture for classification.  
CNN captures spatial features, while LSTM models temporal patterns in emotion transitions.  

---

## ğŸ§  Features  

âœ… End-to-end emotion detection from `.wav` audio  
âœ… Multi-feature extraction with Librosa  
âœ… CNN + BiLSTM architecture  
âœ… Class balancing via computed weights  
âœ… Visualization using Seaborn and Matplotlib  
âœ… Model saving & evaluation support  

---

## ğŸ§ Dataset â€“ RAVDESS  

**Source:** [RAVDESS Emotional Speech Dataset](https://zenodo.org/record/1188976)  

Each audio file name encodes metadata like emotion, intensity, statement, repetition, and actor ID  
(e.g., `03-01-04-01-02-01-09.wav`).

| Code | Emotion    |
|:----:|:-----------|
| 01 | Neutral |
| 02 | Calm |
| 03 | Happy |
| 04 | Sad |
| 05 | Angry |
| 06 | Fearful |
| 07 | Disgust |
| 08 | Surprised |

---

## âš™ï¸ Installation  

```bash
pip install librosa soundfile tensorflow scikit-learn matplotlib seaborn tqdm
````

Set up dataset path in your notebook or script:

```python
DATASET_DIR = "/content/drive/MyDrive/audio lstm cnn"
```

---

## ğŸ§© Model Architecture

```
Input  
 â”œâ”€â”€ Conv2D â†’ BatchNorm â†’ MaxPool â†’ Dropout  
 â”œâ”€â”€ Conv2D â†’ BatchNorm â†’ MaxPool â†’ Dropout  
 â”œâ”€â”€ Conv2D â†’ BatchNorm â†’ MaxPool â†’ Dropout  
 â”œâ”€â”€ Reshape â†’ BiLSTM â†’ Dense(128) â†’ Dropout  
 â””â”€â”€ Dense(8) [Softmax Output]
```

**Hyperparameters**

* Optimizer: Adam
* Loss: Categorical Crossentropy
* Batch Size: 32
* Epochs: 40
* Metrics: Accuracy

---

## ğŸ“Š Results

| Metric            | Value       |
| :---------------- | :---------- |
| **Test Accuracy** | **37.08 %** |
| **Train Samples** | 1200        |
| **Test Samples**  | 240         |

**Classification Report:**

```
              precision    recall  f1-score   support
angry             0.40      0.59      0.47        32
calm              0.60      0.56      0.58        32
disgust           0.28      0.69      0.40        32
fearful           0.55      0.19      0.28        32
happy             0.31      0.12      0.18        32
neutral           0.12      0.12      0.12        16
sad               0.38      0.16      0.22        32
surprised         0.42      0.41      0.41        32
```

**Visualization:**

```python
sns.heatmap(cm, annot=True, fmt="d",
            xticklabels=le.classes_,
            yticklabels=le.classes_)
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()
```

---

## ğŸ’¾ Saved Artifacts

| File                    | Description                                   |
| :---------------------- | :-------------------------------------------- |
| `ravdess_best_model.h5` | Best checkpoint (highest validation accuracy) |
| `ravdess_crnn_model.h5` | Final trained model                           |
| `label_encoder.pkl`     | Label encoder for decoding predictions        |

---

## ğŸš€ Future Scope

ğŸ”¹ Add attention or transformer-based layers
ğŸ”¹ Use SpecAugment for audio data augmentation
ğŸ”¹ Deploy model via FastAPI or Streamlit for real-time inference
ğŸ”¹ Train on multilingual emotion datasets

---

## ğŸ‘¨â€ğŸ’» Author

**Varun Sharma**
ğŸ“ IIIT Bhagalpur
ğŸ’¡ Focused on **Generative AI** and **Deep Learning** applications.

---

## ğŸ Acknowledgements

* [RAVDESS Dataset](https://zenodo.org/record/1188976)
* [Librosa](https://librosa.org/) for audio feature extraction
* [TensorFlow/Keras](https://www.tensorflow.org/) for model implementation

---

â­ **If you found this project useful, consider giving it a star on GitHub!**

```


